{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group Study Season 1-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Group Study Season 1-2: Learning Graph Embedding for Compositional Zero-shot Learning\n",
        "\n",
        "#### The materaials for Group Study in Perception Lab Durham University\n",
        "#### This is the simplified code for paper 'Learning Graph Embedding for Compositional Zero-shot Learning'. In this season, we will see what a graph looks like and how it pass through GNN.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hwLmmDRpiWEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Today's takeaway (3/06/2022)\n",
        "\n",
        "\n",
        "*   A view of a simple graph\n",
        "*   Basic GCN theory\n",
        "*   Compositional zero-shot in GNN\n",
        "\n",
        "> Source code: https://github.com/ExplainableML/czsl\n",
        "\n",
        "####Prerequisites: Please download ***'utzappos-graph.t7'*** from Teams -> Research Repositories -> file/Documents/tools/coding, and drag it into the files manager on the left.\n"
      ],
      "metadata": {
        "id": "HdNZ2-Rsw3NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "cudnn.benchmark = True\n",
        "\n",
        "# Python imports\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from os.path import join as ospj\n",
        "import csv\n",
        "import numpy as np\n",
        "import scipy.sparse as sp"
      ],
      "metadata": {
        "id": "rkImJRW_EK5b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "### Data preparation\n"
      ],
      "metadata": {
        "id": "pVhkz27CBycn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define that there are 16 attritutes and 12 objects in the data, compisiting totally 116 potential pairs, 83 of which are available in the training session."
      ],
      "metadata": {
        "id": "Jsn08F7CaVj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# random generate 1024 image features and labels\n",
        "img_feat = torch.rand(1024, 300)\n",
        "attr = torch.randint(0,16,(1024,))\n",
        "obj = torch.randint(0,12,(1024,))\n",
        "pair = torch.randint(0,83,(1024,))\n",
        "print('total attribute numbers:', 16)\n",
        "print('seen attribute numbers:', len(set(attr.numpy())))\n",
        "print('\\ntotal object numbers:', 12)\n",
        "print('seen object numbers:', len(set(obj.numpy())))\n",
        "print('\\ntotal pairs:', 116)\n",
        "print('seen object numbers:', len(set(pair.numpy())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBYmvt-FDx72",
        "outputId": "3b08aea8-1e81-4425-dde5-029309b9d695"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total attribute numbers: 16\n",
            "seen attribute numbers: 16\n",
            "\n",
            "total object numbers: 12\n",
            "seen object numbers: 12\n",
            "\n",
            "total pairs: 116\n",
            "seen object numbers: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load graph\n",
        "graph = torch.load('/content/utzappos-graph.t7')\n",
        "embeddings = graph['embeddings']  # (144, 300)\n",
        "adj = graph['adj']  # (144, 144)\n",
        "print(type(adj))\n",
        "print(type(embeddings))\n",
        "print(adj)"
      ],
      "metadata": {
        "id": "Wg0GH7NupLH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6efac54-1df8-469b-aff2-8a4418ef2da9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse.coo.coo_matrix'>\n",
            "<class 'torch.Tensor'>\n",
            "  (0, 0)\t1.0\n",
            "  (0, 16)\t1.0\n",
            "  (0, 18)\t1.0\n",
            "  (0, 19)\t1.0\n",
            "  (0, 20)\t1.0\n",
            "  (0, 21)\t1.0\n",
            "  (0, 22)\t1.0\n",
            "  (0, 24)\t1.0\n",
            "  (0, 25)\t1.0\n",
            "  (0, 26)\t1.0\n",
            "  (1, 1)\t1.0\n",
            "  (1, 18)\t1.0\n",
            "  (1, 19)\t1.0\n",
            "  (1, 22)\t1.0\n",
            "  (1, 23)\t1.0\n",
            "  (1, 24)\t1.0\n",
            "  (1, 26)\t1.0\n",
            "  (2, 2)\t1.0\n",
            "  (2, 16)\t1.0\n",
            "  (2, 18)\t1.0\n",
            "  (2, 27)\t1.0\n",
            "  (3, 3)\t1.0\n",
            "  (3, 16)\t1.0\n",
            "  (3, 17)\t1.0\n",
            "  (3, 18)\t1.0\n",
            "  :\t:\n",
            "  (24, 137)\t1.0\n",
            "  (138, 14)\t1.0\n",
            "  (138, 25)\t1.0\n",
            "  (14, 138)\t1.0\n",
            "  (25, 138)\t1.0\n",
            "  (139, 14)\t1.0\n",
            "  (139, 26)\t1.0\n",
            "  (14, 139)\t1.0\n",
            "  (26, 139)\t1.0\n",
            "  (140, 14)\t1.0\n",
            "  (140, 27)\t1.0\n",
            "  (14, 140)\t1.0\n",
            "  (27, 140)\t1.0\n",
            "  (141, 15)\t1.0\n",
            "  (141, 21)\t1.0\n",
            "  (15, 141)\t1.0\n",
            "  (21, 141)\t1.0\n",
            "  (142, 15)\t1.0\n",
            "  (142, 26)\t1.0\n",
            "  (15, 142)\t1.0\n",
            "  (26, 142)\t1.0\n",
            "  (143, 15)\t1.0\n",
            "  (143, 27)\t1.0\n",
            "  (15, 143)\t1.0\n",
            "  (27, 143)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "transfer sparse matrix to tenaor"
      ],
      "metadata": {
        "id": "oq9IH3XvUwto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normt_spm(mx, method='in'):\n",
        "    if method == 'in':\n",
        "        mx = mx.transpose()\n",
        "        rowsum = np.array(mx.sum(1))\n",
        "        r_inv = np.power(rowsum, -1).flatten()\n",
        "        r_inv[np.isinf(r_inv)] = 0.\n",
        "        r_mat_inv = sp.diags(r_inv)\n",
        "        mx = r_mat_inv.dot(mx)\n",
        "        return mx"
      ],
      "metadata": {
        "id": "vNDRkkO1EZWa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spm_to_tensor(sparse_mx):\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(np.vstack(\n",
        "            (sparse_mx.row, sparse_mx.col))).long()\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "metadata": {
        "id": "tvZZyzOKBIYU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, dropout=True, relu=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.layer = nn.Linear(in_channels, out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs, adj):\n",
        "        if self.dropout is not None:\n",
        "            inputs = self.dropout(inputs)\n",
        "\n",
        "        outputs = torch.mm(adj, torch.mm(inputs, self.layer.weight.T)) + self.layer.bias  # â–³\n",
        "\n",
        "        if self.relu is not None:\n",
        "            outputs = self.relu(outputs)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "sdsq3uHu1iEN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define GCN model. Two gnn layers."
      ],
      "metadata": {
        "id": "nAZnucQgLg5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "\n",
        "    def __init__(self, adj):\n",
        "        super().__init__()\n",
        "\n",
        "        adj = normt_spm(adj, method='in')\n",
        "        adj = spm_to_tensor(adj)\n",
        "        self.train_adj = adj\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        conv = GraphConv(300, 4096)\n",
        "        self.add_module('conv0', conv)\n",
        "        layers.append(conv)\n",
        "\n",
        "        conv = GraphConv(4096, 300, relu=False)\n",
        "        self.add_module('conv-last', conv)\n",
        "        layers.append(conv)\n",
        "\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        for conv in self.layers:\n",
        "            x = conv(x, self.train_adj)\n",
        "\n",
        "        return F.normalize(x)"
      ],
      "metadata": {
        "id": "mdb6gZ3hDXyQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcn = GCN(adj)\n",
        "optimizer = optim.Adam(gcn.parameters(), lr=0.0001, weight_decay=0.0001)"
      ],
      "metadata": {
        "id": "Y-IKCb_d6hUh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = True\n",
        "current_embeddings = gcn(embeddings)  # (144, 300)\n",
        "print(current_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Nt4rG5E_U9y",
        "outputId": "964fedea-31e0-4138-9b13-922c10ca9a06"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([144, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the index of 83 seen pairs in 144 attr + obj + pairs "
      ],
      "metadata": {
        "id": "DUS-RZtEVPYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_idx = [ 28,  29,  30,  32,  35,  36,  37,  40,  41,  42,  43,  45,  46,  49,\n",
        "        50,  51,  52,  53,  55,  56,  57,  58,  60,  61,  63,  64,  65,  66,\n",
        "        67,  69,  70,  71,  73,  76,  78,  79,  72,  75,  77,  80,  82,  83,\n",
        "        86,  87,  88,  89,  91,  93,  94,  95,  96,  98,  99, 101, 103, 104,\n",
        "        105, 107, 108, 110, 111, 113, 114, 116, 117, 119, 121, 122, 124, 125,\n",
        "        126, 118, 128, 131, 134, 135, 136, 137, 138, 139, 140, 141, 143]\n",
        "print(\"number of seen pairs: \", len(train_idx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHj4tB13Gqim",
        "outputId": "c50a0ad2-8d43-4184-b2fd-84992014673f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of seen pairs:  83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pick out the seen embeddings\n",
        "\n",
        "do dot product with image feature\n",
        "\n",
        "calculate the loss"
      ],
      "metadata": {
        "id": "QV0yqWXuV7Ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pair_embed = current_embeddings[train_idx]  # (83,300)\n",
        "pair_embed = pair_embed.permute(1,0)  # (300, 83)\n",
        "pair_pred = torch.matmul(img_feat, pair_embed)  # (batch_size, 300) * (300, 83) -> (batch_size, 83)\n",
        "loss = F.cross_entropy(pair_pred, pair)\n",
        "print(\"loss = \", loss.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VGmppwBB1xd",
        "outputId": "e106facc-ff1e-4597-963c-f355d13a8d5a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss =  tensor(4.4417)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "0hKyZMwGEcUr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n",
        "### data preparation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5CzrQAO5FMs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_feat = torch.rand(32, 300)"
      ],
      "metadata": {
        "id": "L8930tzcNnup"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = False\n",
        "gcn.eval()\n",
        "with torch.no_grad():\n",
        "  current_embeddings = gcn(embeddings)\n",
        "  pair_embeds = current_embeddings[28:144,:].permute(1,0)  # (300, 116)\n",
        "  score = torch.matmul(img_feat, pair_embeds)"
      ],
      "metadata": {
        "id": "_3L2ZtEdMg0Y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(score, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MWUGP34Tm1n",
        "outputId": "29fafc97-a106-4768-cab2-bce29da91813"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 95,  89, 101,  ...,  30, 101, 101])\n"
          ]
        }
      ]
    }
  ]
}